marginalWeights[is.na(marginalWeights)] <- 0
# Compute Total Points per OBJECTIVE over years
marginalWeights[, totalObjectiveYears := get(paste0('pointsObjectives', oldYear))
+ get(paste0('pointsObjectives', newYear))]
# Compute Total points per TOPIC over years
marginalWeights[, totalTopicYears := sum(totalObjectiveYears), by = topicsVarName]
###############################
# Time to compute the ratios. #
###############################
# Ratio of Objective on Topic in 2011
marginalWeights[, paste0('ratio', newYear) :=
get(paste0('pointsObjectives', newYear)) /
get(paste0('pointsTopic', newYear))]
# Ratio of Objective on Topic over both years
marginalWeights[, ratioObjectiveBoth := totalObjectiveYears / totalTopicYears]
# Item weight.
marginalWeights[, itemWeight := get(paste0('ratio', newYear)) /
ratioObjectiveBoth]
####################################
# Merge item Weights to each item. #
####################################
itemData <-
merge(itemData, marginalWeights[, list(itemWeight, get(objectivesVarName))],
by.x = objectivesVarName, by.y = 'V2') # here we merge the marginalWeights with itemData (because they are item data)
# Reorder columns so it will be easier to look things up.
setcolorder(itemData, c(2:16, 1, 17:28))
# Also reorder rows by itemID
itemData <- itemData[order(itemID), ]
# Remove Marginal Weights temporary object.
rm(marginalWeights) # and here we get rid of marginalWeights, because it was a temp object.
# Compute reweighted max points
itemData[, reweightedMaxPoints := maxPoints * itemWeight]
# In this file we calculate the thetas for all students' plausible thetas.
# 1. We take a .csv file with the student data (translated from SPSS .sav file)
# 2. Import a .csv with the scaling constant.
# 3. Convert the 5 scale scores to theta, for all students.
############
# Preamble #
############
# Source required code
source(paste0(codeDir, "fRecastThetas.R"))
###############
# Computation #
###############
# Step 1: Get student usage information.
studentData <- fread(paste0(inputDir, studentDataFileName))
# Keep only relevant variables
if (grade == 4) {
varsToKeep <-		c("IDCNTRY", "TOTWGT", "JKZONE", "JKREP",
paste0("ASMMAT", str_pad(1:numberOfPVs, 2, pad = "0")),
paste0("ASMDAT", str_pad(1:numberOfPVs, 2, pad = "0")),
paste0("ASMGEO", str_pad(1:numberOfPVs, 2, pad = "0")),
paste0("ASMNUM", str_pad(1:numberOfPVs, 2, pad = "0")))
} else if (grade == 8) {
varsToKeep <-		c("IDCNTRY", "TOTWGT", "JKZONE", "JKREP",
paste0("BSMMAT", str_pad(1:numberOfPVs, 2, pad = "0")),
paste0("BSMALG", str_pad(1:numberOfPVs, 2, pad = "0")),
paste0("BSMDAT", str_pad(1:numberOfPVs, 2, pad = "0")),
paste0("BSMGEO", str_pad(1:numberOfPVs, 2, pad = "0")),
paste0("BSMNUM", str_pad(1:numberOfPVs, 2, pad = "0")))
}
studentData <- studentData[, ..varsToKeep]
# Step 2: Get scaling constants.
scalingConstants <- fread(paste0(inputDir,scalingConstantsFile))
# Step 3: Compute thetas.
if (analysisMode == 'Domain') {
lapply(names(traitsByTopics), computeThetas)
} else if (analysisMode == 'General' & grade == '4') {
computeThetas('ASMMAT')
} else if (analysisMode == 'General' & grade == '8') {
computeThetas('BSMMAT')
}
rowNames(studentData) <- paste0("student", 1:nrow(studentData))
row.names(studentData) <- paste0("student", 1:nrow(studentData))
head(studentData)
row.names(studentData)
head(studentData)
class(studentData)
View(itemData)
View(studentData)
# In this file we calculate the response probabilities for all student x items.
# We do it over the 5 PVs, which means that for student i we need to compute
# the probability 5 times
#
# To facilitate mapping, we will create 5 matrices, one for each PV.
# Each column will correspond to an item, and each row to a student. So each
# response probability matrix will be i students BY j items (12569 X 249).
############
# Preamble #
############
# Source required code
source(paste0(codeDir, "fIRTfunctions.R"))
###############
# Computation #
###############
# We compute the five matrices.
# We run the computeRP() function five times PER TOPIC. For each plausible value
# we select the appropriate mix of thetas and items, compute the probabilities,
# and bind it into a single matrix. We should end up with 1 matrix per PV, all
# of which have the same number of rows (students = 12569) and columns
# (items = 249).
# We get the names of items that belong to each Topic.
itemGroups <- list()
for (i in 1:length(traitsByTopics)) {
temp <- character()
name <- names(traitsByTopics[i])
for (j in 1:length(traitsByTopics[[i]])) {
temp <- c(temp,
itemData[get(topicsVarName) == traitsByTopics[[i]][j], itemID])
}
itemGroups[[name]] <- temp
}
# And we compute the five matrices with response probabilities, by looping
# over the number of plausible values.
for (i in 1:numberOfPVs) {
temp <-
aggregateRP(names(traitsByTopics), i, analysisMode, itemGroups, nrow(studentData))
assign(paste0("responseProbsPV", i), temp, envir = .GlobalEnv)
}
rm(temp)
# In this file we calculate the expected total scores per topic.
# We do it over the 5 respProbs, which means for student i we will have 5 PV X
# 6 Topics = 30 Expected Total Scores
#
# We will need the response probabilities and item data.
# So, let us do some matrix multiplication. We multiply each row
# in the respProbs matrices (all probs per student) by the weights for each
# item. We then transpose the weighted matrix, so we have students in rows
# and items on columns.
# We do this for all i PVs (in this case 5)
for (i in 1:numberOfPVs) {
name <- paste0("responseProbsPV", i)
# Now, create a temp matrix which will have the response probabilities
# ordered by item name.
temp <<- get(name)[, order(colnames(get(name)))]
# Check that the colnames for the response proability matrix are in the same
# order as the items in the itemData data.table. If not, stop.
if (all.equal(colnames(temp), itemData[order(itemID), itemID]) == TRUE) {
temp <<- apply(temp, 1, FUN = function(x) itemData$itemWeight * x)
temp <<- t(temp)
assign(paste0("weightedRespProb", i), temp)
rm(temp)
} else {
cat("Columns do not match. Quitting.\n")
break()
}
}
# Let's free some space by getting rid of the unweighted probability matrices
rm(list = apropos("^responseProb"))
# Let us select the item names, their weights and reweighted max points.
# We also create a list with the denominators
itemsByTopic <- list()
denominators <- list()
uniqueTopics <- unique(unlist(traitsByTopics, use.names = FALSE))
# We simply loop over the unique Topic Names.
# Create a small data frame with weights to add to itemsByTopic list
for (i in 1:length(uniqueTopics)) {
temp <<- itemData[get(topicsVarName) == uniqueTopics[i],
.(itemID, itemWeight, reweightedPoints = maxPoints * itemWeight)]
itemsByTopic[[uniqueTopics[i]]] <- temp
denominators[[uniqueTopics[i]]] <- as.numeric(sum(temp$reweightedPoints) * .01)
}
rm(temp)
# Now... we want to compute the NUMBER CORRECT expected score and the
# PERCENT expected score. We want to do this for the 1) Total Score,
# 2) three domains, and 3) 6 topics. That means that we need a dataframe with
# 10 columns, times 5 PV = 50 columns.
uniqueTopics <- unique(unlist(traitsByTopics, use.names = FALSE))
colNames <- sapply(uniqueTopics, FUN = function(x) paste(paste0('ExpectedNmbCorr_',
x,1:numberOfPVs), sep = ','),
simplify = TRUE)
colNames <- as.list(colNames)
colNames <- c(colNames, sapply('Total', FUN = function(x) paste(paste0('ExpectedNmbCorr_',
x,1:numberOfPVs), sep = ','),
simplify = TRUE))
colNames <- c(colNames, sapply(names(traitsByTopics), FUN = function(x) paste(paste0('ExpectedNmbCorr_',
x,1:numberOfPVs), sep = ','),
simplify = TRUE))
# Create data frame for number correct scores.
expectedNmbCorr <- as.data.frame(matrix(NA, nrow = nrow(studentData),
ncol = length(colNames)))
colnames(expectedNmbCorr) <- colNames
# Create data frame for percent scores.
expectedPercent <- as.data.frame(matrix(NA, nrow = nrow(studentData),
ncol = length(colNames)))
colNames <- sub("NmbCorr", "Percent", colnames(expectedNmbCorr))
colnames(expectedPercent) <- colNames
rm(colNames)
# Now we loop over the i TOPICS and j PVs in order to compute all
# i * j topic scores.
for (i in seq_along(uniqueTopics)) {
for (j in 1:numberOfPVs) {
colNameNmbCorr <- paste(paste0('ExpectedNmbCorr_',uniqueTopics[i],j), sep = ',')
colNamepercent <- paste(paste0('ExpectedPercent_',uniqueTopics[i],j), sep = ',')
matrixName     <- paste0("weightedRespProb", j)
items          <- itemData[get(topicsVarName) == uniqueTopics[i], itemID]
expectedNmbCorr[, colNameNmbCorr] <- rowSums(get(matrixName)[,items])
expectedPercent[, colNamepercent] <- expectedNmbCorr[, colNameNmbCorr] /
itemData[itemID %in% items, sum(reweightedMaxPoints) * .01]
}
}
# Now we do the total score, over j PVs.
for (j in 1:numberOfPVs) {
colNameNmbCorr <- paste(paste0('ExpectedNmbCorr_Total',j), sep = ',')
colNamepercent <- paste(paste0('ExpectedPercent_Total',j), sep = ',')
matrixName     <- paste0("weightedRespProb", j)
items <- itemData[, itemID]
expectedNmbCorr[, colNameNmbCorr] <- rowSums(get(matrixName)[,items])
expectedPercent[, colNamepercent] <- expectedNmbCorr[, colNameNmbCorr] /
itemData[itemID %in% items, sum(reweightedMaxPoints) * .01]
}
# Now we loop over the i DOMAINS and j PVs in order to compute all
# i * j domain scores.
for (i in seq_along(names(traitsByTopics))) {
for (j in 1:numberOfPVs) {
colNameNmbCorr <- paste(paste0('ExpectedNmbCorr_',names(traitsByTopics)[i],j), sep = ',')
colNamepercent <- paste(paste0('ExpectedPercent_',names(traitsByTopics)[i],j), sep = ',')
topics         <- traitsByTopics[[i]]
matrixName     <- paste0("weightedRespProb", j)
items          <- itemData[topicsVarName %in% topics, itemID]
expectedNmbCorr[, colNameNmbCorr] <- rowSums(get(matrixName)[,items])
expectedPercent[, colNamepercent] <- expectedNmbCorr[, colNameNmbCorr] /
itemData[itemID %in% items, sum(reweightedMaxPoints) * .01]
}
}
View(expectedPercent)
expectedNmbCorr <- cbind(studentData[, ..varsToKeep], expectedNmbCorr)
View(expectedNmbCorr)
# In this file we calculate the expected total scores per topic.
# We do it over the 5 respProbs, which means for student i we will have 5 PV X
# 6 Topics = 30 Expected Total Scores
#
# We will need the response probabilities and item data.
# So, let us do some matrix multiplication. We multiply each row
# in the respProbs matrices (all probs per student) by the weights for each
# item. We then transpose the weighted matrix, so we have students in rows
# and items on columns.
# We do this for all i PVs (in this case 5)
for (i in 1:numberOfPVs) {
name <- paste0("responseProbsPV", i)
# Now, create a temp matrix which will have the response probabilities
# ordered by item name.
temp <<- get(name)[, order(colnames(get(name)))]
# Check that the colnames for the response proability matrix are in the same
# order as the items in the itemData data.table. If not, stop.
if (all.equal(colnames(temp), itemData[order(itemID), itemID]) == TRUE) {
temp <<- apply(temp, 1, FUN = function(x) itemData$itemWeight * x)
temp <<- t(temp)
assign(paste0("weightedRespProb", i), temp)
rm(temp)
} else {
cat("Columns do not match. Quitting.\n")
break()
}
}
# Let's free some space by getting rid of the unweighted probability matrices
rm(list = apropos("^responseProb"))
# Let us select the item names, their weights and reweighted max points.
# We also create a list with the denominators
itemsByTopic <- list()
denominators <- list()
uniqueTopics <- unique(unlist(traitsByTopics, use.names = FALSE))
# We simply loop over the unique Topic Names.
# Create a small data frame with weights to add to itemsByTopic list
for (i in 1:length(uniqueTopics)) {
temp <<- itemData[get(topicsVarName) == uniqueTopics[i],
.(itemID, itemWeight, reweightedPoints = maxPoints * itemWeight)]
itemsByTopic[[uniqueTopics[i]]] <- temp
denominators[[uniqueTopics[i]]] <- as.numeric(sum(temp$reweightedPoints) * .01)
}
rm(temp)
# Now... we want to compute the NUMBER CORRECT expected score and the
# PERCENT expected score. We want to do this for the 1) Total Score,
# 2) three domains, and 3) 6 topics. That means that we need a dataframe with
# 10 columns, times 5 PV = 50 columns.
uniqueTopics <- unique(unlist(traitsByTopics, use.names = FALSE))
colNames <- sapply(uniqueTopics, FUN = function(x) paste(paste0('ExpectedNmbCorr_',
x,1:numberOfPVs), sep = ','),
simplify = TRUE)
colNames <- as.list(colNames)
colNames <- c(colNames, sapply('Total', FUN = function(x) paste(paste0('ExpectedNmbCorr_',
x,1:numberOfPVs), sep = ','),
simplify = TRUE))
colNames <- c(colNames, sapply(names(traitsByTopics), FUN = function(x) paste(paste0('ExpectedNmbCorr_',
x,1:numberOfPVs), sep = ','),
simplify = TRUE))
# Create data frame for number correct scores.
expectedNmbCorr <- as.data.frame(matrix(NA, nrow = nrow(studentData),
ncol = length(colNames)))
colnames(expectedNmbCorr) <- colNames
# Create data frame for percent scores.
expectedPercent <- as.data.frame(matrix(NA, nrow = nrow(studentData),
ncol = length(colNames)))
colNames <- sub("NmbCorr", "Percent", colnames(expectedNmbCorr))
colnames(expectedPercent) <- colNames
rm(colNames)
# Now we loop over the i TOPICS and j PVs in order to compute all
# i * j topic scores.
for (i in seq_along(uniqueTopics)) {
for (j in 1:numberOfPVs) {
colNameNmbCorr <- paste(paste0('ExpectedNmbCorr_',uniqueTopics[i],j), sep = ',')
colNamepercent <- paste(paste0('ExpectedPercent_',uniqueTopics[i],j), sep = ',')
matrixName     <- paste0("weightedRespProb", j)
items          <- itemData[get(topicsVarName) == uniqueTopics[i], itemID]
expectedNmbCorr[, colNameNmbCorr] <- rowSums(get(matrixName)[,items])
expectedPercent[, colNamepercent] <- expectedNmbCorr[, colNameNmbCorr] /
itemData[itemID %in% items, sum(reweightedMaxPoints) * .01]
}
}
# Now we do the total score, over j PVs.
for (j in 1:numberOfPVs) {
colNameNmbCorr <- paste(paste0('ExpectedNmbCorr_Total',j), sep = ',')
colNamepercent <- paste(paste0('ExpectedPercent_Total',j), sep = ',')
matrixName     <- paste0("weightedRespProb", j)
items <- itemData[, itemID]
expectedNmbCorr[, colNameNmbCorr] <- rowSums(get(matrixName)[,items])
expectedPercent[, colNamepercent] <- expectedNmbCorr[, colNameNmbCorr] /
itemData[itemID %in% items, sum(reweightedMaxPoints) * .01]
}
# Now we loop over the i DOMAINS and j PVs in order to compute all
# i * j domain scores.
for (i in seq_along(names(traitsByTopics))) {
for (j in 1:numberOfPVs) {
colNameNmbCorr <- paste(paste0('ExpectedNmbCorr_',names(traitsByTopics)[i],j), sep = ',')
colNamepercent <- paste(paste0('ExpectedPercent_',names(traitsByTopics)[i],j), sep = ',')
topics         <- traitsByTopics[[i]]
matrixName     <- paste0("weightedRespProb", j)
items          <- itemData[topicsVarName %in% topics, itemID]
expectedNmbCorr[, colNameNmbCorr] <- rowSums(get(matrixName)[,items])
expectedPercent[, colNamepercent] <- expectedNmbCorr[, colNameNmbCorr] /
itemData[itemID %in% items, sum(reweightedMaxPoints) * .01]
}
}
expectedNmbCorr <- cbind(studentData[, .c("IDCNTRY", "TOTWGT", "JKZONE", "JKREP")], expectedNmbCorr)
expectedNmbCorr <- cbind(studentData[, .("IDCNTRY", "TOTWGT", "JKZONE", "JKREP")], expectedNmbCorr)
# In this file we calculate the expected total scores per topic.
# We do it over the 5 respProbs, which means for student i we will have 5 PV X
# 6 Topics = 30 Expected Total Scores
#
# We will need the response probabilities and item data.
# So, let us do some matrix multiplication. We multiply each row
# in the respProbs matrices (all probs per student) by the weights for each
# item. We then transpose the weighted matrix, so we have students in rows
# and items on columns.
# We do this for all i PVs (in this case 5)
for (i in 1:numberOfPVs) {
name <- paste0("responseProbsPV", i)
# Now, create a temp matrix which will have the response probabilities
# ordered by item name.
temp <<- get(name)[, order(colnames(get(name)))]
# Check that the colnames for the response proability matrix are in the same
# order as the items in the itemData data.table. If not, stop.
if (all.equal(colnames(temp), itemData[order(itemID), itemID]) == TRUE) {
temp <<- apply(temp, 1, FUN = function(x) itemData$itemWeight * x)
temp <<- t(temp)
assign(paste0("weightedRespProb", i), temp)
rm(temp)
} else {
cat("Columns do not match. Quitting.\n")
break()
}
}
# Let's free some space by getting rid of the unweighted probability matrices
rm(list = apropos("^responseProb"))
# Let us select the item names, their weights and reweighted max points.
# We also create a list with the denominators
itemsByTopic <- list()
denominators <- list()
uniqueTopics <- unique(unlist(traitsByTopics, use.names = FALSE))
# We simply loop over the unique Topic Names.
# Create a small data frame with weights to add to itemsByTopic list
for (i in 1:length(uniqueTopics)) {
temp <<- itemData[get(topicsVarName) == uniqueTopics[i],
.(itemID, itemWeight, reweightedPoints = maxPoints * itemWeight)]
itemsByTopic[[uniqueTopics[i]]] <- temp
denominators[[uniqueTopics[i]]] <- as.numeric(sum(temp$reweightedPoints) * .01)
}
rm(temp)
# Now... we want to compute the NUMBER CORRECT expected score and the
# PERCENT expected score. We want to do this for the 1) Total Score,
# 2) three domains, and 3) 6 topics. That means that we need a dataframe with
# 10 columns, times 5 PV = 50 columns.
uniqueTopics <- unique(unlist(traitsByTopics, use.names = FALSE))
colNames <- sapply(uniqueTopics, FUN = function(x) paste(paste0('ExpectedNmbCorr_',
x,1:numberOfPVs), sep = ','),
simplify = TRUE)
colNames <- as.list(colNames)
colNames <- c(colNames, sapply('Total', FUN = function(x) paste(paste0('ExpectedNmbCorr_',
x,1:numberOfPVs), sep = ','),
simplify = TRUE))
colNames <- c(colNames, sapply(names(traitsByTopics), FUN = function(x) paste(paste0('ExpectedNmbCorr_',
x,1:numberOfPVs), sep = ','),
simplify = TRUE))
# Create data frame for number correct scores.
expectedNmbCorr <- as.data.frame(matrix(NA, nrow = nrow(studentData),
ncol = length(colNames)))
colnames(expectedNmbCorr) <- colNames
# Create data frame for percent scores.
expectedPercent <- as.data.frame(matrix(NA, nrow = nrow(studentData),
ncol = length(colNames)))
colNames <- sub("NmbCorr", "Percent", colnames(expectedNmbCorr))
colnames(expectedPercent) <- colNames
rm(colNames)
# Now we loop over the i TOPICS and j PVs in order to compute all
# i * j topic scores.
for (i in seq_along(uniqueTopics)) {
for (j in 1:numberOfPVs) {
colNameNmbCorr <- paste(paste0('ExpectedNmbCorr_',uniqueTopics[i],j), sep = ',')
colNamepercent <- paste(paste0('ExpectedPercent_',uniqueTopics[i],j), sep = ',')
matrixName     <- paste0("weightedRespProb", j)
items          <- itemData[get(topicsVarName) == uniqueTopics[i], itemID]
expectedNmbCorr[, colNameNmbCorr] <- rowSums(get(matrixName)[,items])
expectedPercent[, colNamepercent] <- expectedNmbCorr[, colNameNmbCorr] /
itemData[itemID %in% items, sum(reweightedMaxPoints) * .01]
}
}
# Now we do the total score, over j PVs.
for (j in 1:numberOfPVs) {
colNameNmbCorr <- paste(paste0('ExpectedNmbCorr_Total',j), sep = ',')
colNamepercent <- paste(paste0('ExpectedPercent_Total',j), sep = ',')
matrixName     <- paste0("weightedRespProb", j)
items <- itemData[, itemID]
expectedNmbCorr[, colNameNmbCorr] <- rowSums(get(matrixName)[,items])
expectedPercent[, colNamepercent] <- expectedNmbCorr[, colNameNmbCorr] /
itemData[itemID %in% items, sum(reweightedMaxPoints) * .01]
}
# Now we loop over the i DOMAINS and j PVs in order to compute all
# i * j domain scores.
for (i in seq_along(names(traitsByTopics))) {
for (j in 1:numberOfPVs) {
colNameNmbCorr <- paste(paste0('ExpectedNmbCorr_',names(traitsByTopics)[i],j), sep = ',')
colNamepercent <- paste(paste0('ExpectedPercent_',names(traitsByTopics)[i],j), sep = ',')
topics         <- traitsByTopics[[i]]
matrixName     <- paste0("weightedRespProb", j)
items          <- itemData[topicsVarName %in% topics, itemID]
expectedNmbCorr[, colNameNmbCorr] <- rowSums(get(matrixName)[,items])
expectedPercent[, colNamepercent] <- expectedNmbCorr[, colNameNmbCorr] /
itemData[itemID %in% items, sum(reweightedMaxPoints) * .01]
}
}
expectedNmbCorr <- cbind(studentData[, .(IDCNTRY, TOTWGT, JKZONE, JKREP)], expectedNmbCorr)
expectedPercent <- cbind(studentData[, .(IDCNTRY, TOTWGT, JKZONE, JKREP)], expectedPercent)
View(expectedPercent)
# This file sets the important directories and global variables. It then calls,
# one by one, all script files that calculate the topic scores.
# Each script file roughly corresponds to a slide on the PowerPoint
# presentation prepared by Enis Dogan.
############
# Preamble #
############
# Make sure there are no objects in memory.
rm(list = ls())
# Set the directory structure. codeDir, inputDir and outpurDir MUST be
# sub directories of baseDir.
baseDir   <- "c:/Users/tcalico/Desktop/TIMSS_topic_october/"
codeDir   <- paste0(baseDir, "Code/")
inputDir  <- paste0(baseDir, "Input/")
outputDir <- paste0(baseDir, "Output/")
# Load required libraries.
library(data.table)
library(stringr)
# For this code to work you need three files: studentdatafile, scalingconstantsfile, and itemdatafile
# 1. studentDataFileName must be named "ASAUSAM6.csv" or "BSAUSAM6" depending on grade
# 2. scalingConstantsFile must be named "ScalingConstants2015TIMSSG8" or "ScalingConstants2011TIMMSG4"
# depending on cycle/grade
# 3. itemDataFileName must be named ""T07T11combinedTIMSSG4.csv" or T011T15combinedTIMSSG8" etc.
# depending on cycle/grade
###############################################################################
# Set global variables
###############################################################################
grade        <- '4'          # 4 or 8
analysisMode <- 'Domain'     # Domain or General
oldYear      <- '2007'       # 2007 or 2011
newYear      <- '2011'       # 2011 or 2015
objectivesVarName <- "Objective"
topicsVarName     <- "Topic"
# Names studentdatafile, itemdatafile, and scaling constants object based on grade and newYear
studentDataFileName  <- "ASAUSAM5.csv"                    # .csv file with student data
itemDataFileName     <- "T07T11combinedTIMSSG4.csv"       # .csv file with item information data.
scalingConstantsFile <- "ScalingConstants2011TIMSSG4.csv" # .csv file with scaling data
# Create a list with the names of the latent traits and the corresponding
# Topic codes. Necessary to map latent trait estimates to items, and then
# compute response probabilities.
if (grade == '4') {
traitsByTopics <- list(ASMDAT = c("D1"),
ASMGEO = c("G1", "G2"),
ASMNUM = c("N1", "N2", "N3"))
} else if (grade == '8') {
traitsByTopics <- list(BSMALG = c("A1", "A2", "A3"),
BSMDAT = c("D1", "D2", "D3"),
BSMGEO = c("G1", "G2", "G3"),
BSMNUM = c("N1", "N2", "N3"))
}
# State how many plausible values exist, in order to automate computation
# of each response probability matrix.
numberOfPVs <- 5
# Decide wether to create files with intermediate computation results.
saveIntermediate <- TRUE #TRUE or FALSE
###############################################################################
###############################################################################
###############
# Computation #
###############
# Now call the files and compute
source(paste0(codeDir, "/2. ComputeItemWeights.R"))
source(paste0(codeDir, "/3. ComputeThetas.R"))
source(paste0(codeDir, "/4. ComputeAnswerProbabilities.R"))
source(paste0(codeDir, "/5. ComputeExpectedScores.R"))
View(expectedPercent)
paste0(runDir, "1. Run_2015_Grd8_General.R")
runDir 		 <- "C:/Users/tcalico/Desktop/TIMSS_topic_october/Code/"
paste0(runDir, "1. Run_2015_Grd8_General.R")
